{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nlgeval import NLGEval\n",
    "\n",
    "nlgeval = NLGEval(no_skipthoughts=True, no_glove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(re.compile('^,$'), ''), (re.compile('^.$'), '')]\n",
      "yes you sucks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class StopwordFilter(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.pats = []\n",
    "        if os.path.exists(filename):\n",
    "            for ln in open(filename, 'r').readlines():\n",
    "                ww = ln.split()\n",
    "                if len(ww)==1:\n",
    "                    self.pats.append((re.compile(r'^' + ww[0] + r'$'), ''))\n",
    "                elif len(ww)==2:\n",
    "                    self.pats.append((re.compile(r'^' + ww[0] + r'$'), ww[1]))\n",
    "\n",
    "    def _filter(self, input_words):\n",
    "        output_words = []\n",
    "        for w in input_words:\n",
    "            target = w\n",
    "            for p in self.pats:\n",
    "                v = p[0].sub(p[1],w)\n",
    "                if v != w:\n",
    "                    target = v\n",
    "                    break\n",
    "            if target != '':\n",
    "                output_words.append(target)\n",
    "        return output_words\n",
    "\n",
    "    def __call__(self, input_words):\n",
    "        if isinstance(input_words, str):\n",
    "            return ' '.join(self._filter(input_words.split()))\n",
    "        elif isinstance(input_words, list):\n",
    "            return self._filter(input_words)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "swfilter = StopwordFilter('punc')\n",
    "print(swfilter.pats)\n",
    "print(swfilter('yes , you sucks .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for file in root.iterdir():\\n    try:\\n        f =  file.open()\\n        data = json.load(f)\\n        metrics_dict = eval(data)\\n        with open('metrics/' + file.name, 'w') as f:\\n            json.dump(metrics_dict, f)\\n        print(file.name)\\n    except:\\n        print('fail')\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "root = Path('generate')\n",
    "def eval(data):\n",
    "    refs = [[]]\n",
    "    hypo = []\n",
    "    for dialog in data['dialogs']:\n",
    "        for qa in dialog['dialog']:\n",
    "            question = qa['question']\n",
    "            ans = qa['answer']\n",
    "            predict = qa['predict']\n",
    "            refs[0].append(swfilter(ans))\n",
    "            hypo.append(swfilter(predict))\n",
    "    metrics_dict = nlgeval.compute_metrics(refs, hypo) \n",
    "    return metrics_dict\n",
    "\n",
    "'''for file in root.iterdir():\n",
    "    try:\n",
    "        f =  file.open()\n",
    "        data = json.load(f)\n",
    "        metrics_dict = eval(data)\n",
    "        with open('metrics/' + file.name, 'w') as f:\n",
    "            json.dump(metrics_dict, f)\n",
    "        print(file.name)\n",
    "    except:\n",
    "        print('fail')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "name = 'test_set_predicted_11141648.json'\n",
    "metric_dict = eval(json.load(open('generate/' + name)))\n",
    "with open('metrics/' + name, 'w') as f:\n",
    "    json.dump(metric_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bleu_1': 0.2704460967213893,\n",
       " 'Bleu_2': 0.1722203552529856,\n",
       " 'Bleu_3': 0.1183584507736195,\n",
       " 'Bleu_4': 0.08518808526572841,\n",
       " 'CIDEr': 0.78996472324051348,\n",
       " 'METEOR': 0.11581314027100997,\n",
       " 'ROUGE_L': 0.29202615766102058}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_released(data, ground):\n",
    "    refs = [[]]\n",
    "    hypo = []\n",
    "    for dialog, g_dialog in zip(data['dialogs'], ground['dialogs']):\n",
    "        for qa, g_qa in zip(dialog['dialog'], g_dialog['dialog']):\n",
    "            question = qa['question']\n",
    "            ans = g_qa['answer']\n",
    "            predict = qa['answer']\n",
    "            refs[0].append(swfilter(ans))\n",
    "            hypo.append(swfilter(predict))\n",
    "    metrics_dict = nlgeval.compute_metrics(refs, hypo) \n",
    "    return metrics_dict\n",
    "data = json.load(open('result_test_set_b5_p1.0.json'))\n",
    "ground = json.load(open('generate/test_set_predicted_10280020.json'))\n",
    "eval_released(data, ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bleu_1': 0.2316765364353914,\n",
       " 'Bleu_2': 0.12436788056021685,\n",
       " 'Bleu_3': 0.07763834895019554,\n",
       " 'Bleu_4': 0.049947666507955665,\n",
       " 'CIDEr': 0.63786497275859277,\n",
       " 'METEOR': 0.11131560813254061,\n",
       " 'ROUGE_L': 0.2356145004554919}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def copy_baseline(data):\n",
    "    refs = [[]]\n",
    "    hypo = []\n",
    "    for dialog in data['dialogs']:\n",
    "        for qa in dialog['dialog']:\n",
    "            question = qa['question']\n",
    "            ans = qa['answer']\n",
    "            predict = qa['question']\n",
    "            refs[0].append(swfilter(ans))\n",
    "            hypo.append(swfilter(predict))\n",
    "    metrics_dict = nlgeval.compute_metrics(refs, hypo) \n",
    "    return metrics_dict\n",
    "data = json.load(open('generate/test_set_predicted_102.json'))\n",
    "copy_baseline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['conv11.json', 'attn_decode.json', 'top_down.json', 'm_stage.json', 'all.json', 'simple.json']\n",
    "results = json.load(open('generate/released_baseline.json'))\n",
    "for dialog in results['dialogs']:\n",
    "    for qa in dialog['dialog']:\n",
    "        qa['relased'] = qa['answer']\n",
    "        \n",
    "for name in names:\n",
    "    with open('generate/' + name) as f:\n",
    "        data = json.load(f)\n",
    "        for d_idx, dialog in enumerate(data['dialogs']):\n",
    "            results['dialogs'][d_idx]['caption'] = dialog['caption']\n",
    "            for q_idx, qa in enumerate(dialog['dialog']):\n",
    "                results['dialogs'][d_idx]['dialog'][q_idx][name] = qa['predict']\n",
    "                results['dialogs'][d_idx]['dialog'][q_idx]['answer'] = qa['answer']\n",
    "json.dump(results, open('result.json', 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
