{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Tsung-Yi Lin <tl483@cornell.edu>\n",
    "# Ramakrishna Vedantam <vrama91@vt.edu>\n",
    "\n",
    "import copy\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange as range\n",
    "import six\n",
    "\n",
    "def precook(s, n=4, out=False):\n",
    "    \"\"\"\n",
    "    Takes a string as input and returns an object that can be given to\n",
    "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
    "    can take string arguments as well.\n",
    "    :param s: string : sentence to be converted into ngrams\n",
    "    :param n: int    : number of ngrams for which representation is calculated\n",
    "    :return: term frequency vector for occuring ngrams\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    counts = defaultdict(int)\n",
    "    for k in range(1,n+1):\n",
    "        for i in range(len(words)-k+1):\n",
    "            ngram = tuple(words[i:i+k])\n",
    "            counts[ngram] += 1\n",
    "    return counts\n",
    "\n",
    "def cook_refs(refs, n=4): ## lhuang: oracle will call with \"average\"\n",
    "    '''Takes a list of reference sentences for a single segment\n",
    "    and returns an object that encapsulates everything that BLEU\n",
    "    needs to know about them.\n",
    "    :param refs: list of string : reference sentences for some image\n",
    "    :param n: int : number of ngrams for which (ngram) representation is calculated\n",
    "    :return: result (list of dict)\n",
    "    '''\n",
    "    return [precook(ref, n) for ref in refs]\n",
    "\n",
    "def cook_test(test, n=4):\n",
    "    '''Takes a test sentence and returns an object that\n",
    "    encapsulates everything that BLEU needs to know about it.\n",
    "    :param test: list of string : hypothesis sentence for some image\n",
    "    :param n: int : number of ngrams for which (ngram) representation is calculated\n",
    "    :return: result (dict)\n",
    "    '''\n",
    "    return precook(test, n, True)\n",
    "\n",
    "class CiderScorer(object):\n",
    "    \"\"\"CIDEr scorer.\n",
    "    \"\"\"\n",
    "\n",
    "    def copy(self):\n",
    "        ''' copy the refs.'''\n",
    "        new = CiderScorer(n=self.n)\n",
    "        new.ctest = copy.copy(self.ctest)\n",
    "        new.crefs = copy.copy(self.crefs)\n",
    "        return new\n",
    "\n",
    "    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n",
    "        ''' singular instance '''\n",
    "        self.n = n\n",
    "        self.sigma = sigma\n",
    "        self.crefs = []\n",
    "        self.ctest = []\n",
    "        self.document_frequency = defaultdict(float)\n",
    "        self.cook_append(test, refs)\n",
    "        self.ref_len = None\n",
    "\n",
    "    def cook_append(self, test, refs):\n",
    "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
    "\n",
    "        if refs is not None:\n",
    "            self.crefs.append(cook_refs(refs))\n",
    "            if test is not None:\n",
    "                self.ctest.append(cook_test(test)) ## N.B.: -1\n",
    "            else:\n",
    "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
    "\n",
    "    def size(self):\n",
    "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
    "        return len(self.crefs)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        '''add an instance (e.g., from another sentence).'''\n",
    "\n",
    "        if type(other) is tuple:\n",
    "            ## avoid creating new CiderScorer instances\n",
    "            self.cook_append(other[0], other[1])\n",
    "        else:\n",
    "            self.ctest.extend(other.ctest)\n",
    "            self.crefs.extend(other.crefs)\n",
    "\n",
    "        return self\n",
    "    def compute_doc_freq(self):\n",
    "        '''\n",
    "        Compute term frequency for reference data.\n",
    "        This will be used to compute idf (inverse document frequency later)\n",
    "        The term frequency is stored in the object\n",
    "        :return: None\n",
    "        '''\n",
    "        for refs in self.crefs:\n",
    "            # refs, k ref captions of one image\n",
    "            for ngram in set([ngram for ref in refs for (ngram,count) in six.iteritems(ref)]):\n",
    "                self.document_frequency[ngram] += 1\n",
    "            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
    "\n",
    "    def compute_cider(self):\n",
    "        def counts2vec(cnts):\n",
    "            \"\"\"\n",
    "            Function maps counts of ngram to vector of tfidf weights.\n",
    "            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n",
    "            The n-th entry of array denotes length of n-grams.\n",
    "            :param cnts:\n",
    "            :return: vec (array of dict), norm (array of float), length (int)\n",
    "            \"\"\"\n",
    "            vec = [defaultdict(float) for _ in range(self.n)]\n",
    "            length = 0\n",
    "            norm = [0.0 for _ in range(self.n)]\n",
    "            for (ngram,term_freq) in six.iteritems(cnts):\n",
    "                # give word count 1 if it doesn't appear in reference corpus\n",
    "                df = np.log(max(1.0, self.document_frequency[ngram]))\n",
    "                # ngram index\n",
    "                n = len(ngram)-1\n",
    "                # tf (term_freq) * idf (precomputed idf) for n-grams\n",
    "                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n",
    "                # compute norm for the vector.  the norm will be used for computing similarity\n",
    "                norm[n] += pow(vec[n][ngram], 2)\n",
    "\n",
    "                if n == 1:\n",
    "                    length += term_freq\n",
    "            norm = [np.sqrt(n) for n in norm]\n",
    "            return vec, norm, length\n",
    "\n",
    "        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n",
    "            '''\n",
    "            Compute the cosine similarity of two vectors.\n",
    "            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n",
    "            :param vec_ref: array of dictionary for vector corresponding to reference\n",
    "            :param norm_hyp: array of float for vector corresponding to hypothesis\n",
    "            :param norm_ref: array of float for vector corresponding to reference\n",
    "            :param length_hyp: int containing length of hypothesis\n",
    "            :param length_ref: int containing length of reference\n",
    "            :return: array of score for each n-grams cosine similarity\n",
    "            '''\n",
    "            delta = float(length_hyp - length_ref)\n",
    "            # measure consine similarity\n",
    "            val = np.array([0.0 for _ in range(self.n)])\n",
    "            for n in range(self.n):\n",
    "                # ngram\n",
    "                for (ngram,count) in six.iteritems(vec_hyp[n]):\n",
    "                    # vrama91 : added clipping\n",
    "                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n",
    "\n",
    "                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n",
    "                    val[n] /= (norm_hyp[n]*norm_ref[n])\n",
    "\n",
    "                assert(not math.isnan(val[n]))\n",
    "                # vrama91: added a length based gaussian penalty\n",
    "                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n",
    "            return val\n",
    "\n",
    "        # compute log reference length\n",
    "        self.ref_len = np.log(float(len(self.crefs)))\n",
    "\n",
    "        scores = []\n",
    "        for test, refs in zip(self.ctest, self.crefs):\n",
    "            # compute vector for test captions\n",
    "            vec, norm, length = counts2vec(test)\n",
    "            # compute vector for ref captions\n",
    "            score = np.array([0.0 for _ in range(self.n)])\n",
    "            for ref in refs:\n",
    "                vec_ref, norm_ref, length_ref = counts2vec(ref)\n",
    "                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n",
    "            # change by vrama91 - mean of ngram scores, instead of sum\n",
    "            score_avg = np.mean(score)\n",
    "            # divide by number of references\n",
    "            score_avg /= len(refs)\n",
    "            # multiply score by 10\n",
    "            score_avg *= 10.0\n",
    "            # append score of an image to the score list\n",
    "            scores.append(score_avg)\n",
    "        return scores\n",
    "\n",
    "    def compute_score(self, option=None, verbose=0):\n",
    "        # compute idf\n",
    "        self.compute_doc_freq()\n",
    "        # assert to check document frequency\n",
    "        assert(len(self.ctest) >= max(self.document_frequency.values()))\n",
    "        # compute cider score\n",
    "        score = self.compute_cider()\n",
    "        # debug\n",
    "        # print score\n",
    "        return np.mean(np.array(score)), np.array(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pdb\n",
    "\n",
    "class Cider:\n",
    "    \"\"\"\n",
    "    Main Class to compute the CIDEr metric \n",
    "    \"\"\"\n",
    "    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n",
    "        # set cider to sum over 1 to 4-grams\n",
    "        self._n = n\n",
    "        # set the standard deviation parameter for gaussian penalty\n",
    "        self._sigma = sigma\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Main function to compute CIDEr score\n",
    "        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n",
    "                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n",
    "        :return: cider (float) : computed CIDEr score for the corpus \n",
    "        \"\"\"\n",
    "\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n",
    "\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref = gts[id]\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) > 0)\n",
    "\n",
    "            cider_scorer += (hypo[0], ref)\n",
    "\n",
    "        (score, scores) = cider_scorer.compute_score()\n",
    "\n",
    "        return score, scores\n",
    "\n",
    "    def method(self):\n",
    "        return \"CIDEr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.load(open('generate/test_set_predicted_09082210.json'))\n",
    "#data = json.load(open('generate/test_set_predicted_09100207.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypos, refs = {}, {}\n",
    "count = 0\n",
    "for video in data['dialogs']:\n",
    "    for qa in video['dialog']:\n",
    "        hypos[count] = [qa['predict']]\n",
    "        refs[count] = [qa['answer']]\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cider = Cider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.90404494904749855,\n",
       " array([  2.72963747e+00,   6.79128119e-01,   8.39630289e+00, ...,\n",
       "          7.69503925e-02,   1.32119212e+00,   7.27218524e-03]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cider.compute_score(hypos, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
